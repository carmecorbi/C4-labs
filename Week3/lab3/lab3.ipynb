{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 3: Reconstruction from two views (knowing internal camera parameters)**\n",
    "\n",
    "In this lab you will learn how to estimate the fundamental matrix that relates two images, corresponding to two different views of the same scene, given a set of correspondences between them. Also, you will learn how to triangulate the matching correspondences between two views of the same scene, by means of applying the Direct Linear Method (DLT). Then, you will compute the camera matrices of these images given the Fundamental Matrix and the Intrinsic Matrix, and will evaluate your triangulation method on these camera matrices by estimating the reprojection error in the triangulation. \n",
    "\n",
    "The following file combines some text cells (Markdown cells) and code cells. Some parts of the code need to be completed. All tasks you need to complete are marked in <span style='color:Green'> green.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import cv2\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (19, 10)\n",
    "from operator import itemgetter\n",
    "from plotly import graph_objects as go\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "from time import time \n",
    "from functools import partial\n",
    "\n",
    "import utils\n",
    "from utils import homogeneous2euclidean, euclidean2homogeneous, projective2img, img2projective\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Estimation of the fundamental matrix**\n",
    "\n",
    "### **1.1 Normalized 8-point algorithm**\n",
    "\n",
    "The first task is to create the function that estimates the fundamental matrix given a set of point correspondences between a pair of images.\n",
    "\n",
    "<span style='color:Green'> - Complete the function \"fundamental_matrix\" below.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_matrix(points1, points2):\n",
    "    \n",
    "    # complete ...\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check that the completed function works properly you may use this code which is a toy example where we know the ground truth image.\n",
    "\n",
    "<span style='color:Green'> - Complete the expression of the ground truth fundamental matrix.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two camera matrices for testing purposes\n",
    "P1 = np.zeros((3,4))\n",
    "P1[0,0]=P1[1,1]=P1[2,2]=1\n",
    "angle = 15\n",
    "theta = np.radians(angle)\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n",
    "t = np.array([[.3, .1, .2]])\n",
    "\n",
    "P2 = np.concatenate((R, t.T), axis=1)\n",
    "n = 8\n",
    "rand = np.random.uniform(0,1,n)\n",
    "rand = rand.reshape((1, n))\n",
    "rand2 = np.random.uniform(0,1,2*n)\n",
    "rand2 = rand2.reshape((2, n))\n",
    "ones = np.ones((1,n))\n",
    "X = np.concatenate((rand2, 3*rand, ones), axis=0)\n",
    "\n",
    "x1_test = P1 @ X\n",
    "x2_test = P2 @ X\n",
    "\n",
    "# Estimate fundamental matrix (you need to create this function)\n",
    "F_es = fundamental_matrix(x1_test, x2_test)\n",
    "\n",
    "# Ground truth fundamental matrix \n",
    "F_gt = # complete ...\n",
    "\n",
    "# Evaluation: these two matrices should be very similar\n",
    "F_gt = np.sign(F_gt[0,0])*F_gt / LA.norm(F_gt)\n",
    "F_es = np.sign(F_es[0,0])*F_es / LA.norm(F_es)\n",
    "print(F_gt)\n",
    "print(F_es)\n",
    "print(LA.norm(F_gt-F_es))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Robust estimation of the fundamental matrix**\n",
    "\n",
    "The goal of this section is to estimate the fundamental matrix in a practical situation where the image correspondences contain outliers. For that you will have to write the code of the robust version of the previous algorithm.\n",
    "\n",
    "We will start by computing and visualizing the image matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inage correspondences #\n",
    "\n",
    "# Read images\n",
    "img1 = cv2.imread('Data/0000_s.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/0001_s.png',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create(3000)\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Show \"good\" matches\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> - Write the function Ransac_fundamental_matrix that embeds in a RANSAC procedure the previous DLT algorithm for estimating the fundamental matrix. You may use the provided RANSAC function in lab 2 as a starting point.  </span>\n",
    "\n",
    "Note: In order not to end up, eventually, in an infinite loop, it is recommended to set, at each iteration, the maximum number of iterarions to the maximum between a predefined maximum number of iterations by the user and the automatically estimated max. number of iterarions. The estimated number of iterations ensures we pick, with a probability of 0.99, an initial set of correspondences with no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ransac_fundamental_matrix(points1, points2, th, max_it_0):\n",
    "    \n",
    "    # complete ...\n",
    "    \n",
    "    return F, inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust estimation of the fundamental matrix #\n",
    "points1 = []\n",
    "points2 = []\n",
    "for m in matches:\n",
    "    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n",
    "    points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n",
    "    \n",
    "points1 = np.asarray(points1)\n",
    "points1 = points1.T\n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "\n",
    "F, indices_inlier_matches = Ransac_fundamental_matrix(points1, points2, 1, 5000)\n",
    "inlier_matches = itemgetter(*indices_inlier_matches)(matches)\n",
    "\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.imshow(img_12)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Epipolar lines**\n",
    "\n",
    "Now the fundamental matrix has been estimated we are going to display some points and their corresponding epipolar lines.\n",
    "\n",
    "<span style='color:Green'> - Complete the code that computes the epipolar lines in both images.  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = # epipolar lines in image 2 (complete) \n",
    "l1 = # epipolar lines in image 1 (complete) \n",
    "\n",
    "# choose three random indices\n",
    "N = indices_inlier_matches.shape[0]\n",
    "indices = random.sample(range(1, N), 3)\n",
    "\n",
    "m1 = indices_inlier_matches[indices[0]]\n",
    "m2 = indices_inlier_matches[indices[1]]\n",
    "m3 = indices_inlier_matches[indices[2]]\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from utils import line_draw, plot_img\n",
    "\n",
    "img_path = \"./Data/0000_s.png\"\n",
    "I = Image.open(img_path)\n",
    "size = I.size\n",
    "canv = ImageDraw.Draw(I)\n",
    "line_draw(l1[:,m1], canv, size)\n",
    "line_draw(l1[:,m2], canv, size)\n",
    "line_draw(l1[:,m3], canv, size)\n",
    "canv.ellipse((round(points1[0,m1]), round(points1[1,m1]), round(points1[0,m1])+7, round(points1[1,m1])+7), fill = 'red', outline ='red')\n",
    "canv.ellipse((round(points1[0,m2]), round(points1[1,m2]), round(points1[0,m2])+7, round(points1[1,m2])+7), fill = 'red', outline ='red')\n",
    "canv.ellipse((round(points1[0,m3]), round(points1[1,m3]), round(points1[0,m3])+7, round(points1[1,m3])+7), fill = 'red', outline ='red')\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plot_img(I)\n",
    "\n",
    "img_path = \"./Data/0001_s.png\"\n",
    "I2 = Image.open(img_path)\n",
    "size = I2.size\n",
    "canv2 = ImageDraw.Draw(I2)\n",
    "line_draw(l2[:,m1], canv2, size)\n",
    "line_draw(l2[:,m2], canv2, size)\n",
    "line_draw(l2[:,m3], canv2, size)\n",
    "canv2.ellipse((round(points2[0,m1]), round(points2[1,m1]), round(points2[0,m1])+7, round(points2[1,m1])+7), fill = 'red', outline ='red')\n",
    "canv2.ellipse((round(points2[0,m2]), round(points2[1,m2]), round(points2[0,m2])+7, round(points2[1,m2])+7), fill = 'red', outline ='red')\n",
    "canv2.ellipse((round(points2[0,m3]), round(points2[1,m3]), round(points2[0,m3])+7, round(points2[1,m3])+7), fill = 'red', outline ='red')\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plot_img(I2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Triangulation with the DLT method**\n",
    "\n",
    "The first task is to implement the DLT method that will compute the 3D points corresponding to the matches of two images whose camera matrices are known. [See derivation of DLT here](https://www.cs.cmu.edu/~16385/s17/Slides/11.4_Triangulation.pdf)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/8356912/151347636-c3332917-a7da-402e-9eb6-f39beb91591a.png)\n",
    "\n",
    "\n",
    "<span style='color:Green'> - Create the function triangulate(x1, x2, P1, P2, imsize) that performs a triangulation with the homogeneous algebraic method (DLT)  </span>\n",
    "\n",
    "The entries are (x1, x2, P1, P2, imsize), where:\n",
    "- x1: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 1 reference frame \n",
    "- x2: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 2 reference frame.\n",
    "- P1: Camera 1 matrix of shape (3,4)\n",
    "- P2: Camera 2 matrix of shape (3,4)\n",
    "- imsize: Iterable with shape (2,) with the image size\n",
    "\n",
    "The function should return:\n",
    "- X: array of shape (4, `n_points`), containing the Homogenous Coordinates of the points in 3D space.  \n",
    "\n",
    "Test the triangulate function: use this code to validate that the function triangulate works properly\n",
    "\n",
    "HINT: check numpy.linalg.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here the method for DLT triangulation\n",
    "def triangulate(x1, x2, P1, P2, imsize) -> np.ndarray:\n",
    "    assert P1.shape == (3,4) == P2.shape\n",
    "    assert x1.shape == x2.shape and x1.shape[0] == 3\n",
    "    '''\n",
    "    Your implementation here\n",
    "    '''\n",
    "    raise NotImplementedError(\"You must implement this, don't forget\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the triangulation function with two Cameras and some random points. The average triangulation error should be lower than 10<sup>-8</sup>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "P1 = np.zeros((3,4))\n",
    "P1[0,0]=P1[1,1]=P1[2,2]=1                            # Cam1 Canonical Camera Matrix\n",
    "\n",
    "angle = 15\n",
    "theta = np.radians(angle)\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])     # Cam2 Rotation_z(theta[deg])\n",
    "t = np.array([[1.1, .1, .2]])                         # Cam2 Translation(x[m], y[m], z[m]) \n",
    "P2 = np.concatenate((R, t.T), axis=1)\n",
    "\n",
    "n_points = 20                                        # Number of data samples (points) (x,y,z)\n",
    "X_eucl = np.random.uniform(0, 1, (3,n_points))       # Random points in 3D space on world reference frame\n",
    "X_eucl[2,:] += 3                                     # Make points fit into the camera frame \n",
    "X = euclidean2homogeneous(X_eucl)                    # \n",
    "\n",
    "# Obtain the camera coordinates (rays) from each point.\n",
    "x_proj1 = P1 @ X                                     # Project to Cam1 image coordinates (x,y,z)->[x_c1:y_c1:z_c1]\n",
    "x_proj2 = P2 @ X                                     # Project to Cam2 image coordinates (x,y,z)->[x_c2:y_c2:z_c2]\n",
    "\n",
    "# Estimate the 3D points (you need to create this function)\n",
    "X_trian = triangulate(x_proj1, x_proj2, P1, P2, ((2,2)));\n",
    "assert X_trian.shape == (4, n_points)\n",
    "\n",
    "# Evaluation: compute the reprojection error\n",
    "X_eucl_pred = homogeneous2euclidean(X_trian)\n",
    "X_eucl = homogeneous2euclidean(X)\n",
    "errors = np.linalg.norm(X_eucl - X_eucl_pred, axis=0)\n",
    "avg_error = np.mean(errors)\n",
    "print(f\"Average triangulation error: {avg_error:.2e}[m]\")\n",
    "assert avg_error < 1e-8, \"Something might be wrong here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will plot the results to check that the triangulated values are close to the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "utils.draw_points(X_eucl.T, ax=ax, c=\"k\", alpha=0.5, s=100, depthshade=True, label=\"Groud Truth\")\n",
    "utils.draw_points(X_eucl_pred.T, ax=ax, c=\"r\", alpha=1, s=5, depthshade=True, label=\"Triangulated\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "fig.add_trace(go.Scatter3d(x=X_eucl[0,:], y=X_eucl[1,:], z=X_eucl[2,:], mode='markers', marker=dict(size=6, color=\"black\", opacity=0.5), name='Ground Truth'))\n",
    "fig.add_trace(go.Scatter3d(x=X_eucl_pred[0,:], y=X_eucl_pred[1,:], z=X_eucl_pred[2,:], mode='markers', marker=dict(size=2, color=\"red\", opacity=1), name='Triangulated'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Reconstruction from two views**\n",
    "\n",
    "The goal of this section is to estimate the 3D reconstruction from two views in a practical situation where the image correspondences contain outliers. \n",
    "\n",
    "To acomplish this you have to:\n",
    "\n",
    "1. Find a set of point correspondences between the two images. This correspondences will contain noise/outliers, therefore we cannot use [Epipolar Geometry](https://www.cs.cmu.edu/~16385/s17/Slides/12.1_Epipolar_Geometry.pdf) to calculate the [Essential Matrix](https://www.cs.cmu.edu/~16385/s17/Slides/12.2_Essential_Matrix.pdf), but we can find the Fundamental Matrix. \n",
    "2. Estimate the [Fundamental Matrix](https://www.cs.cmu.edu/~16385/s17/Slides/12.3_Fundamental_Matrix.pdf) relating the two camera images. Since correspondances are noisy we will use the [8-point Algorithm](https://www.cs.cmu.edu/~16385/s17/Slides/12.4_8Point_Algorithm.pdf) in combination with the RANSAC algorithm, to find a good estimate of $F$.\n",
    "3. Calculate the Essential Matrix from the Fundamental Matrix\n",
    "4. Find the two Camera Matrices.\n",
    "### **3.1 Estimate the image matches**\n",
    "(Code is provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images\n",
    "img1 = cv2.imread('Data/0001_s.png',cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('Data/0002_s.png',cv2.IMREAD_GRAYSCALE)\n",
    "h, w = img1.shape\n",
    "\n",
    "# Initiate ORB detector\n",
    "orb = cv2.ORB_create(3000)\n",
    "# find the keypoints and descriptors with ORB\n",
    "kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "# Keypoint matching\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "\n",
    "# Show \"good\" matches\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "fig = plt.figure()\n",
    "plt.imshow(img_12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Estimate the Fundamental Matrix**\n",
    "To find the Fundamental Matrix $F$ we will use the 8-point algorithm. However, using only 8 correspondences might lead to bad estimates of $F$. To find a robust estimate we will use the RANSAC algorithm. In simple words the algorithm samples randomly 8 correspondences and estimates $F_{estimated}$ for these samples, then it evaluates how well does $F_{estimated}$ fit the rest of the data. We repeat this process until a limit of iterations is reached and select the best estimated $F$.  \n",
    "\n",
    "(Code is provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust estimation of the fundamental matrix #\n",
    "points1 = []\n",
    "points2 = []\n",
    "for m in matches:\n",
    "    points1.append([kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1], 1])\n",
    "    points2.append([kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1], 1])\n",
    "    \n",
    "points1 = np.asarray(points1)\n",
    "points1 = points1.T\n",
    "points2 = np.asarray(points2)\n",
    "points2 = points2.T\n",
    "\n",
    "F, indices_inlier_matches = Ransac_fundamental_matrix(points1, points2, 1, 5000)\n",
    "inlier_matches = itemgetter(*indices_inlier_matches)(matches)\n",
    "\n",
    "img_12 = cv2.drawMatches(img1,kp1,img2,kp2,inlier_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "fig = plt.figure()\n",
    "plt.imshow(img_12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Estimate the Essential Matrix**\n",
    "<span style='color:Green'> - Compute the [Essential matrix](https://www.cs.cmu.edu/~16385/s17/Slides/12.2_Essential_Matrix.pdf) from the Fundamental matrix </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera calibration matrix\n",
    "K = np.array([[2362.12, 0, 1520.69], [0, 2366.12, 1006.81], [0, 0, 1]])\n",
    "scale = 0.3;\n",
    "H = np.array([[scale, 0, 0], [ 0, scale, 0], [0, 0, 1]])\n",
    "K = H @ K;\n",
    "\n",
    "def essential_from_fundamental(K, F):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "E = essential_from_fundamental(K, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4. Estimate the Camera Matrices from the Essential Matrix (Optional)**\n",
    "<span style='color:Green'> - Estimate the camera projection matrices. Assuming the first Camera has a cannonical matrix you will obtain four possible Camera Matrices for the second camera, two rotations and two translations.</span>\n",
    "\n",
    " - Please explain how do you obtain these estimates, not what algorithm or methodology you used to obtain them. i.e., What system of equations are you solving?\n",
    " - Why do you obtain 4 solutions? Why not only one?\n",
    "\n",
    "HINT: Once you estimate the rotations $R$ and translation $T$ of the second camera matrix, you may get as results improper rotations: $|R| = -1$ i.e., $R \\in O(3), R \\notin SO(3)$. <span style='color:Green'> (Why do you think this happens?) </span> In that case you should make the rotation proper by avoiding the unwanted reflexion (multiply by -1 so $|R| = 1$): e.g.,\n",
    "  \n",
    "      if det(R) < 0:  \n",
    "          R = -R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # camera projection matrix for the first camera\n",
    "# P1 = ...\n",
    "\n",
    "# # four possible matrices for the second camera\n",
    "# Pc2 = [None, None, None, None];\n",
    "# Pc2[0] = ...\n",
    "# Pc2[1] = ...\n",
    "# Pc2[2] = ...\n",
    "# Pc2[3] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the 4 possible Camera positions and orientations, in relation to Camera 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first camera and the four possible solutions for the second figure:\n",
    "fig = plt.figure()\n",
    "axs = [fig.add_subplot(1,5,i+1, projection='3d') for i in range(5)]\n",
    "titles = [\"Cam 1\"] + [r\"Cam $2_{%s}$\" % int(i+1) for i in range(len(Pc2)) ]\n",
    "\n",
    "low_lim, up_lim = -1.5, 1.5\n",
    "for ax, title, P in zip(axs, titles, [P1] + Pc2):\n",
    "    utils.plot_camera(P,w,h, 1, ax=ax)\n",
    "    utils.plot_camera(P1,w,h, 1, ax=ax, alpha=0.3, color=\"k\") # Show Cam1 for reference\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(low_lim, up_lim)\n",
    "    ax.set_ylim(low_lim, up_lim)\n",
    "    ax.set_zlim(low_lim, up_lim)\n",
    "    \n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d')\n",
    "for P, color in zip(Pc2, ['r','k','b','g']):\n",
    "    utils.plot_camera(P,w,h, 1, ax=ax, color=color)\n",
    "utils.plot_camera(P1,w,h, 1, ax=ax, alpha=0.3, color=\"k\") # Show Cam1 for reference\n",
    "ax.set_xlim(low_lim, up_lim)\n",
    "ax.set_ylim(low_lim, up_lim)\n",
    "ax.set_zlim(low_lim, up_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:Green'> - Triangulate the correspondences and select the \"optimal\" camera, how do you select it ?</span>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('Data/0000_s.png',cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "# Obtain the inlier points projective coordinates for Camera 1 and 2 \n",
    "x1 = ...\n",
    "x2 = ...\n",
    "\n",
    "# Prepare figure \n",
    "fig = plt.figure(figsize=(15,15))\n",
    "axs = [fig.add_subplot(2,2,i+1, projection='3d') for i in range(4)]\n",
    "\n",
    "# Variable for optimal camera\n",
    "P2_selected = None\n",
    "\n",
    "low_lim, up_lim = -10, 10\n",
    "for i, ax, P2 in zip(range(len(Pc2)), axs, Pc2):\n",
    "    # Estimate by triangulization the 3D coordinates of the matches\n",
    "    X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "    X_eucl = homogeneous2euclidean(X_pred)\n",
    "    \n",
    "    # Render the 3D point cloud\n",
    "    x_img = np.transpose(x1[:2]).astype(int)\n",
    "    rgb_txt = (img1[x_img[:,1], x_img[:,0]])/255\n",
    "    utils.draw_points(X_eucl.T, color=rgb_txt, ax=ax)\n",
    "    utils.plot_camera(P1,w,h,1, ax=ax)\n",
    "    utils.plot_camera(P2,w,h,1, ax=ax)\n",
    "    ax.set_title(r\"Cam $2_{%d}$\" % (i+1))\n",
    "    ax.set_xlim(low_lim, up_lim)\n",
    "    ax.set_ylim(low_lim, up_lim)\n",
    "    ax.set_zlim(low_lim, up_lim)\n",
    "    \n",
    "    # Select the best camera using some metric \n",
    "    some_metric = True:\n",
    "    if some_metric:\n",
    "        P2_selected = P2\n",
    "\n",
    "# From now own P2 will be the camera you selected\n",
    "P2 = P2_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.5 3D Visualization (Optiona, provided)**\n",
    " We will visualize the 3D points for the selected camera with an interactive plot so that you can see the surfaces reconstructed in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n",
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n",
    "# Estimate by triangulization the 3D coordinates of the matches\n",
    "X_pred = triangulate(x1, x2, P1, P2, [w, h])\n",
    "X_eucl = X_pred/X_pred[-1,:]\n",
    "points3d = X_eucl[:3].T\n",
    "fig = go.Figure(layout=dict(height=400, width=550))\n",
    "points3d = X_eucl[:3].T\n",
    "fig.add_trace(go.Scatter3d(x=points3d[:,0], y=points3d[:,1], z=points3d[:,2],mode='markers',name='3d points',marker=dict(color=rgb_txt, size=2)))\n",
    "fig.show()\n",
    "#### And these are the corresponding points in 2D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_selected_points = copy.deepcopy(img1)\n",
    "img_with_selected_points[x_img[:,1], x_img[:,0]] = [255,0,0]\n",
    "plt.figure(figsize = (80,40))\n",
    "plt.imshow(img_with_selected_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.6 Reprojection Error (Optional)**\n",
    "\n",
    "Once you selected a Camera, analyze the error of reprojecting the 3D space back to image coordinates. \n",
    "\n",
    "<span style='color:Green'>- Compute the reprojection errors  </span>\n",
    "\n",
    "<span style='color:Green'>- Plot the histogram of reprojection errors, </span>\n",
    "\n",
    "<span style='color:Green'>- Plot the mean reprojection error  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reprojection_errors(x1, x2, P1, P2, X):\n",
    "    \"\"\"\n",
    "    - x1: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 1 reference frame \n",
    "    - x2: array of shape (3, `n_points`), containing matching points in projective coordinates from Camera 2 reference frame.\n",
    "    - P1: Camera 1 matrix\n",
    "    - P2: Camera 2 matrix \n",
    "    - X: (4, `n_points`) homogenous coordinates of points in 3D space\n",
    "    Compute the reprojection error from `X` to each of the camera's projective coordinates\n",
    "    @return: (cam1_errors, cam2_errors)\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    Your implementation here\n",
    "    '''\n",
    "    raise NotImplementedError(\"You must implement this, don't forget\")\n",
    "\n",
    "    return cam1_errors, cam2_errors\n",
    "\n",
    "def plot_error_stats(cam1_errors, cam2_errors):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,4))\n",
    "    \n",
    "    ax1.hist(cam1_errors, color='c')\n",
    "    ax1.grid(axis='y', alpha=0.2)\n",
    "    ax1.set_xlabel(\"Reprojection Error Cam 1 [What units am I?]\")\n",
    "    ax2.hist(cam2_errors, color='k')\n",
    "    ax2.grid(axis='y', alpha=0.2)\n",
    "    ax2.set_xlabel(\"Reprojection Error Cam 2 [What units am I?]\")\n",
    "\n",
    "\n",
    "# Get projections with selected Camera \n",
    "X_pred = ...\n",
    "\n",
    "cam1_errors, cam2_errors = reprojection_errors(x1, x2, P1, P2, X_pred)\n",
    "\n",
    "plot_error_stats(cam1_errors, cam2_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
